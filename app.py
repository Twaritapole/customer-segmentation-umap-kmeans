# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yt9D9KU7JV-Rkj8awc21XnkpgJEGShJ-
"""

# app.py — Advanced Customer Segmentation Deployment
# Requires: streamlit, pandas, numpy, scikit-learn, seaborn, matplotlib, joblib, openpyxl (for xlsx)
# Optional: umap-learn

import streamlit as st
import pandas as pd
import numpy as np
import os, joblib
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA

# Try import UMAP
try:
    import umap.umap_ as umap
    UMAP_AVAILABLE = True
except Exception:
    umap = None
    UMAP_AVAILABLE = False

# ----------------- Page config and CSS -----------------
st.set_page_config(page_title="Advanced Customer Segmentation", layout="wide")
st.markdown("""
<style>
    body { background-color: #000000; color: #FFFFFF; }
    .stApp { background-color: #000000; color: #FFFFFF; }
    .section-header {
        font-size: 22px; font-weight:700; color:#FFFFFF;
        border-bottom: 2px solid #00BFFF; padding-bottom:6px; margin-top:18px;
    }
    .sub-text { color: #cfcfcf; }
    .metric-card { background-color:#111111; padding:12px; border-radius:8px; }
</style>
""", unsafe_allow_html=True)
sns.set_style("darkgrid")

# ----------------- Helpers -----------------
@st.cache_data
def load_pickle_df(pkl_name="final_customer_segments.pkl"):
    if os.path.exists(pkl_name):
        return joblib.load(pkl_name)
    return None

@st.cache_data
def load_model(path):
    if os.path.exists(path):
        return joblib.load(path)
    return None

def save_models(km, reducer, scaler):
    joblib.dump(km, "kmeans_model.joblib")
    if reducer is not None:
        joblib.dump(reducer, "umap_reducer.joblib")
    joblib.dump(scaler, "scaler.joblib")

def df_to_bytes(df):
    return df.to_csv(index=False).encode("utf-8")

def basic_preprocess(df):
    df = df.copy()
    df.drop_duplicates(inplace=True)

    if "Year_Birth" in df.columns:
        try:
            df["Year_Birth"] = pd.to_numeric(df["Year_Birth"], errors="coerce")
            df["Age"] = pd.Timestamp("now").year - df["Year_Birth"]
        except Exception:
            pass

    if "Kidhome" in df.columns and "Teenhome" in df.columns:
        df["Kids_Teens"] = df[["Kidhome", "Teenhome"]].sum(axis=1)

    if "FamilySize" not in df.columns:
        if "Kidhome" in df.columns and "Teenhome" in df.columns and "NumAdults" in df.columns:
            df["FamilySize"] = df["Kidhome"] + df["Teenhome"] + df["NumAdults"]
        elif "Kidhome" in df.columns and "Teenhome" in df.columns:
            df["FamilySize"] = 2 + df[["Kidhome","Teenhome"]].sum(axis=1)

    mnt_cols = [c for c in df.columns if c.startswith("Mnt")]
    if mnt_cols:
        df["TotalSpent"] = df[mnt_cols].sum(axis=1)

    accepted_cols = [c for c in df.columns if c.lower().startswith("acceptedcmp")]
    if accepted_cols:
        df["TotalAcceptedCmp"] = df[accepted_cols].sum(axis=1)

    if "Dt_Customer" in df.columns:
        try:
            df["Dt_Customer"] = pd.to_datetime(df["Dt_Customer"], errors="coerce")
            df["CustomerForYears"] = (pd.Timestamp("now") - df["Dt_Customer"]).dt.days / 365.25
        except Exception:
            pass

    if "TotalSpent" in df.columns and "Income" in df.columns:
        df["SpendingEfficiency"] = df["TotalSpent"] / df["Income"].replace({0: np.nan})
        df["SpendingEfficiency"] = df["SpendingEfficiency"].fillna(0)

    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df[num_cols] = df[num_cols].fillna(df[num_cols].median())

    cat_cols = df.select_dtypes(include=["object","category"]).columns.tolist()
    df[cat_cols] = df[cat_cols].fillna("Unknown")

    return df

def compute_cluster_metrics(X_repr, labels):
    metrics = {}
    if len(set(labels)) > 1:
        try:
            metrics['silhouette'] = float(silhouette_score(X_repr, labels))
        except Exception:
            metrics['silhouette'] = np.nan
        try:
            metrics['calinski_harabasz'] = float(calinski_harabasz_score(X_repr, labels))
        except Exception:
            metrics['calinski_harabasz'] = np.nan
        try:
            metrics['davies_bouldin'] = float(davies_bouldin_score(X_repr, labels))
        except Exception:
            metrics['davies_bouldin'] = np.nan
    else:
        metrics['silhouette'] = np.nan
        metrics['calinski_harabasz'] = np.nan
        metrics['davies_bouldin'] = np.nan
    return metrics

def radar_chart(cluster_profile, cluster_idx, features):
    values = cluster_profile.loc[cluster_idx, features].values.flatten().tolist()
    min_vals = cluster_profile[features].min()
    max_vals = cluster_profile[features].max()
    norm = [(v - min_vals[i]) / (max_vals[i] - min_vals[i] + 1e-9) for i,v in enumerate(values)]
    angles = np.linspace(0, 2*np.pi, len(features), endpoint=False).tolist()
    norm += norm[:1]
    angles += angles[:1]
    fig, ax = plt.subplots(figsize=(5,5), subplot_kw=dict(polar=True))
    ax.plot(angles, norm, 'o-', linewidth=2)
    ax.fill(angles, norm, alpha=0.25)
    ax.set_thetagrids(np.degrees(angles[:-1]), features)
    ax.set_ylim(0,1)
    ax.set_title(f"Cluster {cluster_idx} profile (normalized)", color="white")
    ax.tick_params(colors='white')
    ax.grid(color='gray', linestyle='--', alpha=0.3)
    return fig

def auto_insights(cluster_profile):
    insights = []
    for idx, row in cluster_profile.iterrows():
        diffs = row - cluster_profile.mean()
        top = diffs.sort_values(ascending=False).head(2)
        bottom = diffs.sort_values().head(1)
        top_features = ", ".join([f"{f} ({row[f]:.1f})" for f in top.index])
        bottom_feature = f"{bottom.index[0]} ({row[bottom.index[0]]:.1f})"
        insights.append(f"Cluster {idx}: higher in {top_features}; lower in {bottom_feature}.")
    return insights

# ----------------- Sidebar: mode & data -----------------
st.sidebar.title("Configuration")
st.sidebar.write("App will attempt to auto-load your Project_1 outputs if present.")

pkl_df = load_pickle_df("final_customer_segments.pkl")
kmeans_saved = load_model("kmeans_model.joblib") or load_model("kmeans_model.pkl")
umap_saved = load_model("umap_reducer.joblib") or load_model("umap_reducer.pkl")
scaler_saved = load_model("scaler.joblib") or load_model("scaler.pkl")

if pkl_df is not None:
    default_mode = "Use project outputs (auto)"
else:
    default_mode = "Upload dataset"

mode = st.sidebar.radio("Mode", (default_mode, "Upload dataset", "Custom clustering (run here)"))

if mode == "Use project outputs (auto)" and pkl_df is None:
    st.sidebar.warning("Saved outputs not found — upload dataset or use Custom clustering.")
    mode = "Upload dataset"

df = None
if mode == "Use project outputs (auto)":
    df = pkl_df.copy()
    st.sidebar.success("Loaded final dataframe from final_customer_segments.pkl")
else:
    uploaded = st.sidebar.file_uploader("Upload CSV or Excel file", type=["csv","xlsx","xls"])
    if uploaded is None:
        st.sidebar.info("Upload a dataset to continue (or place final_customer_segments.pkl in app folder).")
    else:
        if str(uploaded.name).lower().endswith(".csv"):
            df = pd.read_csv(uploaded)
        else:
            df = pd.read_excel(uploaded)
        st.sidebar.success("Uploaded dataset loaded")


st.title("Know Your Customers — Advanced Dashboard")
st.markdown("<div class='sub-text'>This app uses the same preprocessing and clustering logic as your Project_1 notebook. You can view saved results or run custom clustering.</div>", unsafe_allow_html=True)

st.markdown('<div class="section-header">Feature Engineering Summary</div>', unsafe_allow_html=True)
st.markdown("""
- `Kids_Teens` = Kidhome + Teenhome (if present)
- `TotalSpent` = sum of all `Mnt*` columns (if any)
- `TotalAcceptedCmp` = sum of AcceptedCmp1..5 (if present)
- `CustomerForYears` = current date - Dt_Customer (if present)
- `SpendingEfficiency` = TotalSpent / Income (safe division)
- Numeric missing values filled with median; categorical with "Unknown"
""", unsafe_allow_html=True)

df_proc = None # Initialize df_proc
if df is None:
    st.info("No data loaded. Please upload a file or ensure saved project outputs exist.")
else:
    df_proc = basic_preprocess(df)

# Wrap the rest of the application logic in a conditional block
if df_proc is not None:
    if st.sidebar.checkbox("Show raw data (top 10 rows)", value=False):
        st.markdown('<div class="section-header">Dataset Preview</div>', unsafe_allow_html=True)
        st.dataframe(df_proc.head(10))

    st.markdown('<div class="section-header">Select Features for Clustering</div>', unsafe_allow_html=True)
    all_numeric = df_proc.select_dtypes(include=[np.number]).columns.tolist()
    default_feats = [c for c in ["Income","Age","TotalSpent","Kids_Teens","FamilySize","TotalAcceptedCmp","CustomerForYears","SpendingEfficiency"] if c in all_numeric]
    features = st.multiselect("Numeric features (choose at least 2):", options=all_numeric, default=default_feats)

    if len(features) < 2:
        st.warning("Please select at least 2 numeric features to proceed.")
        st.stop()

    X = df_proc[features].copy()
    X = X.fillna(X.median())

    scale_method = st.sidebar.selectbox("Scaling method:", ("standard","minmax"), index=0)
    scaler = StandardScaler() if scale_method == "standard" else MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    st.markdown('<div class="section-header">UMAP & KMeans Controls</div>', unsafe_allow_html=True)
    use_umap = st.checkbox("Use UMAP for reduction before clustering (recommended)", value=UMAP_AVAILABLE and (umap_saved is None))
    umap_n = st.slider("UMAP n_neighbors", 5, 100, 15)
    umap_min = st.slider("UMAP min_dist", 0.0, 0.99, 0.1)
    umap_comp = st.selectbox("UMAP components", (2,3))
    k_val = st.slider("K (number of clusters)", 2, 12, 4)
    run_btn = st.button("Run clustering now" if mode != "Use project outputs (auto)" else "Recompute clusters (override)")

    # ---------------- Saved results section ----------------
    if mode == "Use project outputs (auto)":
        st.markdown('<div class="section-header">Results from Project_1 (Saved)</div>', unsafe_allow_html=True)
        if 'Cluster' in df_proc.columns:
            df_show = df_proc.copy()
            counts = df_show['Cluster'].value_counts().sort_index()
            c1, c2, c3 = st.columns([1.5,1,1])
            with c1:
                st.metric("Clusters", len(counts))
            with c2:
                try:
                    if umap_saved is not None and scaler_saved is not None:
                        X_for_eval = df_show[features].fillna(df_show[features].median())
                        X_scaled_saved = scaler_saved.transform(X_for_eval)
                        X_repr = umap_saved.transform(X_scaled_saved) if UMAP_AVAILABLE and umap_saved is not None else X_scaled_saved
                        sil = silhouette_score(X_repr, df_show['Cluster'])
                        st.metric("Silhouette", f"{sil:.3f}")
                    else:
                        st.write("")
                except Exception:
                    st.write("")
            with c3:
                st.metric("Records", df_show.shape[0])
            st.dataframe(counts.rename("Count").to_frame())

            numeric_cols_for_profile = [c for c in df_show.select_dtypes(include=[np.number]).columns.tolist() if c != 'Cluster']
            if numeric_cols_for_profile:
                profile = df_show.groupby('Cluster')[numeric_cols_for_profile].mean()
                st.markdown('<div class="section-header">Automatic Insights</div>', unsafe_allow_html=True)
                insights = auto_insights(profile)
                for ins in insights:
                    st.write("•", ins)
            else:
                st.info("No numeric columns available for profiling after excluding 'Cluster'.")
        else:
            st.warning("Saved data does not contain a 'Cluster' column. Run custom clustering if needed.")

    # ---------------- Custom clustering ----------------
    if run_btn:
        use_saved_models = st.sidebar.checkbox("Use saved models for predict (if available)", value=False)
        reducer = None
        km = None
        used_repr = None

        if use_saved_models and kmeans_saved is not None and umap_saved is not None and scaler_saved is not None:
            st.info("Using saved scaler, UMAP reducer and KMeans to predict clusters on selected features.")
            try:
                X_for_pred = df_proc[features].fillna(df_proc[features].median())
                X_scaled_saved = scaler_saved.transform(X_for_pred)
                used_repr = umap_saved.transform(X_scaled_saved) if UMAP_AVAILABLE and umap_saved is not None else X_scaled_saved
                labels = kmeans_saved.predict(used_repr)
                km = kmeans_saved
                reducer = umap_saved
                scaler = scaler_saved
            except Exception as e:
                st.error("Failed to use saved models: " + str(e))
                st.stop()
        else:
            if use_umap and UMAP_AVAILABLE:
                with st.spinner("Computing UMAP embedding..."):
                    reducer = umap.UMAP(n_components=umap_comp, n_neighbors=int(umap_n), min_dist=float(umap_min), random_state=42)
                    used_repr = reducer.fit_transform(X_scaled)
            else:
                used_repr = X_scaled

            with st.spinner("Running KMeans..."):
                km = KMeans(n_clusters=int(k_val), random_state=42, n_init=10)
                labels = km.fit_predict(used_repr)

        df_result = df_proc.copy()
        df_result['Cluster'] = labels
        metrics = compute_cluster_metrics(used_repr, labels)

        st.markdown('<div class="section-header">Clustering Results</div>', unsafe_allow_html=True)
        c1, c2, c3 = st.columns([1.5,1,1])
        c1.metric("Clusters", len(set(labels)))
        c2.metric("Silhouette", f"{metrics['silhouette']:.3f}" if not np.isnan(metrics['silhouette']) else "N/A")
        c3.metric("Records", df_result.shape[0])
        st.dataframe(df_result['Cluster'].value_counts().sort_index().rename("Count").to_frame())

        st.markdown('<div class="section-header">Cluster Visualization</div>', unsafe_allow_html=True)
        fig, ax = plt.subplots(figsize=(8,6))
        if used_repr is not None and used_repr.shape[1] >= 2:
            x_plot, y_plot = used_repr[:,0], used_repr[:,1]
        else:
            pca = PCA(n_components=2)
            proj = pca.fit_transform(X_scaled)
            x_plot, y_plot = proj[:,0], proj[:,1]
        palette = sns.color_palette("Set2", len(set(labels)))
        sns.scatterplot(x=x_plot, y=y_plot, hue=labels, palette=palette, s=50, ax=ax, edgecolor="k", linewidth=0.3)
        ax.set_title("Cluster map", color="white")
        ax.tick_params(colors="white")
        st.pyplot(fig)

        st.markdown('<div class="section-header">Cluster Profiles (means)</div>', unsafe_allow_html=True)
        numeric_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()
        if 'Cluster' in numeric_cols:
            numeric_cols.remove('Cluster')
        if numeric_cols:
            profile = df_result.groupby('Cluster')[numeric_cols].mean()
            st.dataframe(profile.round(2))

            st.markdown('<div class="section-header">Radar Chart per Cluster</div>', unsafe_allow_html=True)
            small_features = [c for c in numeric_cols if df_result[c].nunique() > 5][:6]
            if small_features:
                cluster_choice = st.selectbox("Choose cluster for radar chart:", sorted(df_result['Cluster'].unique()))
                fig_r = radar_chart(profile, cluster_choice, small_features)
                st.pyplot(fig_r)
            else:
                st.info("Not enough features for radar chart.")

            st.markdown('<div class="section-header">Automatic Insights</div>', unsafe_allow_html=True)
            insights = auto_insights(profile)
            for ins in insights:
                st.write("•", ins)

        st.markdown('<div class="section-header">Save & Download</div>', unsafe_allow_html=True)
        if st.button("Download clustered CSV"):
            st.download_button("Click to download", df_to_bytes(df_result), file_name="clustered_from_app.csv", mime="text/csv")
        if st.checkbox("Save models (scaler, umap reducer, kmeans) to files"):
            try:
                save_models(km, reducer, scaler)
                st.success("Saved models: kmeans_model.joblib, umap_reducer.joblib (if used), scaler.joblib")
            except Exception as e:
                st.error(f"Failed to save models: {e}")

        st.info("Clustering finished. Use the downloads or saved models for inference elsewhere.")